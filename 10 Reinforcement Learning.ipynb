{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNFwObzKfjiD"
   },
   "source": [
    "# Tutorial 10 - Intro to Reinforcement Learning: Multi-armed bandits\n",
    "\n",
    "*Written and revised by Jozsef Arato, Mengfan Zhang, Dominik Pegler*  \n",
    "Computational Cognition Course, University of Vienna  \n",
    "https://github.com/univiemops/tewa1-computational-cognition\n",
    "\n",
    "---\n",
    "**This tutorial will cover:**\n",
    "\n",
    "- the simulation of a human subject who tries to win the most money while playing on 3 bandits for N trials.\n",
    "\n",
    "---\n",
    "\n",
    "![images.jpg](data:image/jpeg;base64,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)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVadA6CHgJmo"
   },
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3Vwr4KNUyUu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import optimize, stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbYzPcG0U0BO"
   },
   "source": [
    "Set up initial parameters of simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbrSvRabUy5_"
   },
   "outputs": [],
   "source": [
    "n_bandit = 3  # number of bandits\n",
    "p_reward = np.array([0.8, 0.2, 0.4])  # probabilty of reward for each bandit\n",
    "reward = 5  # amount of reward (same for all bandits) for wins"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8fJWC6MQ9Sf"
   },
   "source": [
    "Implement the Softmax choice function\n",
    "\n",
    "the goal of this function is to turn values into probabilites, with a single parameter Temp (Temperature), that controls the explore-exploit trade-off, with higher temperatures corresponding to more exploration\n",
    "\n",
    "https://en.wikipedia.org/wiki/Softmax_function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPfZYUrOQ86u"
   },
   "outputs": [],
   "source": [
    "def soft_max(vals, temp=1):  # default value for temperature is 1\n",
    "    return np.exp(vals / temp) / np.sum(np.exp(vals / temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwGs0tUISWZK"
   },
   "source": [
    "# Test Softmax Function\n",
    "with different values, but the same temperatue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyVNiZAWhMb9",
    "outputId": "cee18a04-617a-4519-c7ed-f57d3d2964c8"
   },
   "outputs": [],
   "source": [
    "test_val1 = np.array([0.8, 1.3, 0.5])\n",
    "test_val2 = np.array([0.8, 1.5, 0.5])\n",
    "\n",
    "print(soft_max(test_val1))\n",
    "print(soft_max(test_val2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TEdNQpp4Q9d"
   },
   "source": [
    "# Test Softmax Function\n",
    "with same values, but different temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rw5ozttuSww3",
    "outputId": "b312ce2a-63c4-460a-d305-e572406a5071"
   },
   "outputs": [],
   "source": [
    "print(1, soft_max(test_val1, 1))\n",
    "print(4, soft_max(test_val1, 4))\n",
    "print(0.5, soft_max(test_val1, 0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W51K5uGYY3xd"
   },
   "source": [
    "# Simulate a 3 armed bandit Experiment\n",
    "\n",
    " NTrials: number of trials (and visualize results)\n",
    "\n",
    "we want to simulate the choice and the learning of the values\n",
    "\n",
    "in this simulation, we have to randomize 2 things:\n",
    "1. which bandit is chosen (based on choice probabilities defined by softmax)\n",
    "2. whether there is a reward (based on the reward probabilities defined at the top of the notebook in P_reward).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXfYYZQnWttX"
   },
   "source": [
    "### as a first step simulate a single trial of choosing among 3 bandits with our choice policy  (based on equal initial values)\n",
    " we can use the np.random.multinomial  to make a random choice among 3 options-- the choice is not uniformly random, but uses the choice probabilities from the policy!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jwJLIUmvWq-2",
    "outputId": "44289e3b-1b0a-4f44-9298-27b984c9c84b"
   },
   "outputs": [],
   "source": [
    "vt = np.ones(n_bandit)  # 3 random or uniform starting values np.random.normal(1,1,3)\n",
    "print(vt)\n",
    "choice_p = soft_max(\n",
    "    vt\n",
    ")  ## use the softmax here with the values estiamtes and some tempereature , eg=1\n",
    "print(choice_p)\n",
    "print(np.random.multinomial(1, choice_p))  # simulate a choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GNbyMw5k1wPp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Z2qGCQ3Xlyf"
   },
   "source": [
    "### multiple trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g_hmfwJ269kq",
    "outputId": "0bc1acb3-37bc-4bd8-c76c-f53474227dba"
   },
   "outputs": [],
   "source": [
    "vt = np.array([1, 5, 1])  # 3 random or uniform starting values np.random.normal(1,1,3)\n",
    "choice_p = soft_max(vt, Temp=1)\n",
    "for i in range(20):\n",
    "    print(np.random.multinomial(1, choice_p))  # simulate a choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SwIU3x47aeUY"
   },
   "source": [
    "### once we have chosen, we have to simulate the random reward\n",
    "\n",
    "\n",
    "you can randomize the actual reward with np.random.rand\n",
    "\n",
    "use P_reward as the probability of reward for each choice\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z47TCGjv8W45",
    "outputId": "8aede52c-945a-4e7e-b516-fc8b542fa012"
   },
   "outputs": [],
   "source": [
    "print(p_reward)\n",
    "choice = np.random.multinomial(1, choiceP)\n",
    "print(choice)\n",
    "\n",
    "print(np.argmax(choice))\n",
    "\n",
    "rand = np.random.rand()\n",
    "print(rand)\n",
    "print(p_reward[np.argmax(choice)])  # probabilty of reward for each bandit\n",
    "if rand < p_reward[np.argmax(choice)]:\n",
    "    print(reward)\n",
    "else:\n",
    "    print(0)\n",
    "\n",
    "# YOUR CODE\n",
    "# YOUR CODE\n",
    "# YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6zm8MM0A_u4"
   },
   "outputs": [],
   "source": [
    "for i in range(50):\n",
    "    rand = np.random.rand()\n",
    "    # print(rand)\n",
    "    # print(p_reward[np.argmax(choice)])  # probabilty of reward for each bandit\n",
    "    if rand < p_reward[np.argmax(choice)]:\n",
    "        reward_trial = 5\n",
    "    else:\n",
    "        reward_trial = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ywb4IyKyC4Uh",
    "outputId": "827b10f6-90a0-42fd-c039-ee2537cbe6a1"
   },
   "outputs": [],
   "source": [
    "choice = np.random.multinomial(1, choiceP)\n",
    "print(choice)\n",
    "if np.random.rand() < p_reward[np.where(choice == 1)]:\n",
    "    reward = 5\n",
    "else:\n",
    "    reward = 0\n",
    "print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dGGraW3WDvp2",
    "outputId": "33bdd4b9-5266-425b-e8d5-fadb264aeba4"
   },
   "outputs": [],
   "source": [
    "vt = np.ones(n_bandit)\n",
    "for i in range(3):\n",
    "    print(\"vt\", vt)\n",
    "    vtNew = np.copy(vt)\n",
    "    vtNew[1] = 0.3\n",
    "    print(\"vtNew\", vtNew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2xrnT0fsEfiR",
    "outputId": "04af193d-9975-4bad-e913-a77d3a9629a6"
   },
   "outputs": [],
   "source": [
    "vt = 5\n",
    "for i in range(3):\n",
    "    print(\"vt\", vt)\n",
    "    vtN = vt\n",
    "    vtN *= 0.3\n",
    "    print(\"vtN\", vtN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "YM35jEBGanq-",
    "outputId": "9518fc4e-65c5-47eb-9f1d-d3564363a631"
   },
   "outputs": [],
   "source": [
    "plt.hist(np.random.rand(10000))\n",
    "plt.plot([0.2, 0.2], [0, 1000], color=\"k\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LyUiEDvzYMoW"
   },
   "source": [
    "Set the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sa6IM-4ZA-Wd"
   },
   "outputs": [],
   "source": [
    "alpha = 0.5  # learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDLvrXE7a3gB"
   },
   "source": [
    "### after getting the reward the \"learning\" happens: the value update with the learning rate alpha (go back to the slides if you need a reminder).\n",
    "the key is that you only update the values for the choice that was made\n",
    "\n",
    "### single trial of reinforcement learning:\n",
    "\n",
    "you have to update Vt for the bandit that was chosen on each trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3TmsrU-QcB2M",
    "outputId": "4ce05886-1518-4561-c0cb-b95953d5a0cb"
   },
   "outputs": [],
   "source": [
    "vt = np.ones(3)  # 3 random or uniform starting values\n",
    "print(\"Initial values\", vt)\n",
    "choice_p = soft_max(vt, Temp=1)\n",
    "print(\"choice P\", choice_p)\n",
    "choice = np.random.multinomial(1, choice_p)\n",
    "print(choice)\n",
    "ch_loc = np.argmax(choice)\n",
    "print(\"choice\", ch_loc)\n",
    "rand = np.random.rand()\n",
    "print(rand)\n",
    "if rand < p_reward[ch_loc]:\n",
    "    reward = 5\n",
    "else:\n",
    "    reward = 0\n",
    "print(\"reward\", reward)\n",
    "vt[ch_loc] = vt[ch_loc] + alpha * (reward - vt[ch_loc])\n",
    "print(\"updated values\", vt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSXEXaLmanGX"
   },
   "source": [
    "### putting it together for an experiment with multiple trials\n",
    "\n",
    "you will need a for cycle, and will have to use the numpy array choices and rewards with indexing to store for each trial the choice and the obtained reward.\n",
    "\n",
    "if you code works, the visualization code should run without any changes  (use n for the trial number)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "w_CkpJcWU4b9",
    "outputId": "7dd4b1bd-c2fa-4901-b378-21ac8105afbd"
   },
   "outputs": [],
   "source": [
    "n_trial = 100  # number of trials\n",
    "vt = np.ones(n_bandit)  # intialize starting values at 1  - uniform\n",
    "cols = [\"r\", \"g\", \"b\"]\n",
    "alpha = 0.01  # learning rate\n",
    "temperature = 1  # temperature\n",
    "fig, ax = plt.subplots(2, 1)\n",
    "rewards = np.zeros(n_trial)  # store obtained rewards\n",
    "choices = np.intp(np.zeros(n_trial))  # store choices\n",
    "for tr in range(n_trial):\n",
    "    choice_p = soft_max(vt, temperature)  # # choice probability\n",
    "    choice = np.random.multinomial(1, choice_p)  # randomization of choice\n",
    "    choices[tr] = np.argmax(choice)  # store choice\n",
    "    rand_rew = np.random.rand()  # randomize reward\n",
    "    if rand_rew < p_reward[choices[tr]]:\n",
    "        rewards[tr] = 5  # store reward\n",
    "    # else:\n",
    "    #   rewards[tr]=0\n",
    "    vt[choices[tr]] = vt[choices[tr]] + alpha * (\n",
    "        rewards[tr] - vt[choices[tr]]\n",
    "    )  # value update/learning\n",
    "\n",
    "    # visualization starts here\n",
    "    for i in range(n_bandit):\n",
    "        ax[0].scatter(tr, vt[i], color=cols[i])\n",
    "    ax[1].scatter(tr, choices[tr], color=cols[choices[tr]])\n",
    "\n",
    "ax[0].set_ylabel(\"Estimated Value\")\n",
    "ax[1].set_ylabel(\"choice\")\n",
    "ax[0].set_title(\"Total Reward = \" + str(np.sum(rewards)))\n",
    "ax[1].set_title(\"Num Best choice = \" + str(np.sum(choices == 0)))\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZrSAMP5__oM"
   },
   "source": [
    "# Function for Experiment Simulation\n",
    "\n",
    "take 3 inputs:\n",
    "1. Learning rate\n",
    "2. Temperature\n",
    "3. number of trials\n",
    "\n",
    "\n",
    "return 3 things:\n",
    "\n",
    "1. Choices\n",
    "2. Obtained rewards\n",
    "3. estimated reward values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fpYgeHmCdBah"
   },
   "outputs": [],
   "source": [
    "def sim_exp(alpha, temp, n_trial):\n",
    "    vt = np.ones(n_bandit)\n",
    "    rewards = np.zeros(n_trial)  # store obtained rewards\n",
    "    choices = np.intp(np.zeros(n_trial))  # store choices\n",
    "    for tr in range(n_trial):\n",
    "        choice_p = soft_max(vt, temp)  # # choice probability\n",
    "        choice = np.random.multinomial(1, choice_p)  # randomization of choice\n",
    "        choices[tr] = np.argmax(choice)  # store choice\n",
    "        rand_rew = np.random.rand()  # randomize reward\n",
    "        if rand_rew < p_reward[choices[tr]]:\n",
    "            rewards[tr] = 5  # store reward\n",
    "        vt[choices[tr]] = vt[choices[tr]] + alpha * (\n",
    "            rewards[tr] - vt[choices[tr]]\n",
    "        )  # value update/learning\n",
    "    return choices, rewards, vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ3B27v4kMQE"
   },
   "source": [
    "# Simulate Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GNgD4AF9WK8r",
    "outputId": "0f9a6a61-651a-4e67-9ed8-afb41994693d"
   },
   "outputs": [],
   "source": [
    "alpha = 0.2\n",
    "temp = 3\n",
    "n_trial = 100\n",
    "choices, rewards, vt = sim_exp(alpha, temp, n_trial)\n",
    "print(\n",
    "    \"Best Choice: \",\n",
    "    np.sum(choices == 0),\n",
    "    \"/\",\n",
    "    n_trial,\n",
    "    \"Average Reward: \",\n",
    "    np.mean(rewards),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5Gh_5bTrLtSW",
    "outputId": "76e1884d-2719-4961-9afa-074af32c37f8"
   },
   "outputs": [],
   "source": [
    "choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L69pHP3VLvjW",
    "outputId": "dc0622a4-51ac-4c11-a128-392a398de1f5"
   },
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrfc_Tckpi-n"
   },
   "source": [
    "\n",
    "\n",
    "# **Switch Perspective,**\n",
    "we have the data of the experiment, we want to analyze the simulated participants\n",
    "(get underyling parameters, that could best explain their behavior)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ejg8AqTVkZ8I"
   },
   "source": [
    "# Calculate Likelihood of Data (choices+ rewards) for some parameters alpha/temp\n",
    "4 input\n",
    "1. Guess for Alpha\n",
    "2. Guess for temperature\n",
    "3. sequence of choices\n",
    "4. sequence of obtained rewards\n",
    "\n",
    "output:\n",
    "likelihood for each trial   \n",
    "\n",
    "the idea here is that we know what the participant chose and the reward received, we want to figure how likely those choices are by assuming some learning rate and choice policy temperature\n",
    "\n",
    "\n",
    "likelihood p(choice sequence|alpha,temperature,rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iyZ04l_ri2hu"
   },
   "outputs": [],
   "source": [
    "def rl_likelihood(alpha, temp, choices, rewards):\n",
    "    n_trial = len(choices)\n",
    "    likelihood_tr = np.zeros(n_trial)\n",
    "    vt = np.ones(n_bandit)\n",
    "    for n in range(n_trial):\n",
    "        choice_p = soft_max(vt, temp)\n",
    "        likelihood_tr[n] = choice_p[choices[n]]\n",
    "        vt[choices[n]] = vt[choices[n]] + alpha * (rewards[n] - vt[choices[n]])\n",
    "    return likelihood_tr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "Gr44dlYSkhBU",
    "outputId": "d0db434f-af93-4e77-c9ac-27fee01c5da7"
   },
   "outputs": [],
   "source": [
    "likelihoods = rl_likelihood(0.05, 1, choices, rewards)\n",
    "\n",
    "plt.scatter(np.arange(len(choices)), choices, c=likelihoods)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tCclb4K8Jt2a",
    "outputId": "92d09231-dae2-4b18-9a4e-9e54d2b59979"
   },
   "outputs": [],
   "source": [
    "likelihoods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jjx4ZT0VAVaP"
   },
   "source": [
    "# Modify the above function, so that it returns the total negative log likelihood\n",
    "(you only need to change the ouptut, the calculation is the same\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hmr9fxrrATNB"
   },
   "outputs": [],
   "source": [
    "def rl_log_l(alpha, temp, choices, rewards):\n",
    "    n_trial = len(choices)\n",
    "    likelihood_tr = np.zeros(n_trial)\n",
    "    vt = np.ones(n_bandit)\n",
    "    for n in range(n_trial):\n",
    "        choice_p = soft_max(vt, temp)\n",
    "        likelihood_tr[n] = choice_p[choices[n]]\n",
    "        vt[choices[n]] = vt[choices[n]] + alpha * (rewards[n] - vt[choices[n]])\n",
    "    return -np.sum(np.log(likelihood_tr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cks9EtWmk_51",
    "outputId": "9164f9d2-0470-4a5b-f948-5b19ce3179a1"
   },
   "outputs": [],
   "source": [
    "rl_log_l(0.01, 3, choices, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FpdF1gb8L7Q5",
    "outputId": "8b543b1b-1b85-4d06-b57c-6b61bb0b817e"
   },
   "outputs": [],
   "source": [
    "rl_log_l(0.01, 10, choices, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ikqhxQVfMCGy",
    "outputId": "067644f7-5366-4723-cc4c-a1df0ffebc0c"
   },
   "outputs": [],
   "source": [
    "rl_log_l(0.05, 1, choices, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "g7o7zhE3MINj",
    "outputId": "857e81f2-c77f-4a1d-dca9-d00a91220321"
   },
   "outputs": [],
   "source": [
    "rl_log_l(0.05, 0.5, choices, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4U3-Gg7MPOK",
    "outputId": "d4baf40b-8f19-4339-cd1d-1a2db7b9b132"
   },
   "outputs": [],
   "source": [
    "rl_log_l(0.1, 0.5, choices, rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSdwKbukA6pF"
   },
   "source": [
    "# test different values of alpha and temperature and calculate the log-likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7_ryoqv5lKox",
    "outputId": "cf8fad18-76ec-4be0-b85a-f3c2ca3f4fc4"
   },
   "outputs": [],
   "source": [
    "rl_log_l(alpha, temp, choices, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6ZSAx1Cflyuk",
    "outputId": "a8848d79-e6c5-4245-ee80-167b378b6cb3"
   },
   "outputs": [],
   "source": [
    "rl_log_l(0.1, 10, choices, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IclPssUSeri5"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7idGdcb0er2A"
   },
   "source": [
    "# Plot Relationship between alpha and log likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 300
    },
    "id": "4wq0EOdPpdyo",
    "outputId": "0d1ce00c-6056-46e0-e4a8-6ab79c7671b9"
   },
   "outputs": [],
   "source": [
    "alphas = np.linspace(0.01, 0.2, 12)\n",
    "\n",
    "plt.figure()\n",
    "# YOUR CODE\n",
    "x = plt.xticks(np.arange(len(alphas)), np.round(alphas, 2))\n",
    "plt.ylabel(\"-log Likelihood\", fontsize=14)\n",
    "plt.xlabel(\"Learning Rate\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jt_SjK_eGJYL"
   },
   "source": [
    "It is better to automatize the above 'search' process\n",
    "--> numerical optimization based on maximum likelihood to find best alpha-temperature best parameter combination\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fSuq5_uqBFc"
   },
   "source": [
    "# Model fitting- maximum likelihood\n",
    "\n",
    "we need to modify the log likelihood function, so that it can be used with an optimizer (scipy.minimize)\n",
    "\n",
    "the only trick is passing the parameters and arguments in combined arrays,\n",
    "the rest is the same as the negative log likelihood funciton above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CRxTq8nPqMhn"
   },
   "outputs": [],
   "source": [
    "def RL_fit_logL(pars,args):\n",
    "  Alpha,Temp=pars[0],pars[1]\n",
    "  Choices,Rewards=args[0],args[1]\n",
    "  NTrial=len(Choices)\n",
    "  LikelihoodTr=np.zeros(NTrial)\n",
    "  Vt=np.ones(NBandit)*.5\n",
    "  for n in range(NTrial):\n",
    "    ChoiceP=SoftMax(Vt,Temp)\n",
    "    LikelihoodTr[n]= # YOUR CODE-key line\n",
    "    Vt[Choices[n]]=Vt[Choices[n]]+Alpha*(Rewards[n]-Vt[Choices[n]])\n",
    "  return -np.sum(np.log(LikelihoodTr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tqK74DrquGp"
   },
   "outputs": [],
   "source": [
    "?optimize.minimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p1Ue98BJ11Fp"
   },
   "source": [
    "# run optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z4xOq1ImuTD1",
    "outputId": "d1318e7c-61ea-4895-8de8-7024998b81ae"
   },
   "outputs": [],
   "source": [
    "optimize.minimize(rl_fit_log_l, x0=[0.5, 0.5], args=[choices, rewards])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3jWNZPUgAwm3"
   },
   "source": [
    "Change the number of trials in the simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnYN2p3SAxUM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Gz4-1RDuUd_"
   },
   "source": [
    "# Run optimizer with bounds\n",
    "x is the final value of optimization\n",
    "for reliable model fitting we need to pass bounds for the optimizaiton\n",
    "(as negative learning rate and temperature do not make sense)\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-UPr5CRuqoDw",
    "outputId": "c9a25dc9-88e4-49c5-a244-0fe638c68bd6"
   },
   "outputs": [],
   "source": [
    "optimize.minimize(\n",
    "    rl_fit_log_l, x0=[0.5, 0.5], args=[choices, rewards], bounds=[(0, 1), (0, 5)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F5oLksOY1w38"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yprUWy0kx-GC"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55lDwNvPHPfb"
   },
   "source": [
    " ## Homework  1\n",
    "\n",
    " Changing environments: above reward probability is fixed , making the task of the 'experimental participant' relatively easy, since once you know the best option, you can just always choose it.\n",
    "\n",
    " Try to modify the code above, to make an experimental simulation where the rewards probabilities of the bandits change over time (but stay within the range 0-1).  \n",
    "\n",
    "Simulate 300 trials, where there is a change every 50 trials, when the reward reward probabilty values change (and also the identity of the best bandit)\n",
    "\n",
    "Explore different values of alpha and temperature in this experiment, and visualize your results (good values are the ones that lead to a high total reward).\n",
    "\n",
    "Visualize the outcome (including when the change in value occurs). As an inspiration, you could use the figure on the lecture slide 8. However that is for 1 bandit only.\n",
    "\n",
    "Make 3 figures, where you show  how 3 different values of alpha (.01,.03,.1) affect the leanring of the reward probability.\n",
    "Which one seems the best in this changing environment?\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ty-QuxJrh_Ug"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ECiJaKUxIak2"
   },
   "source": [
    "## Homework 2\n",
    "modify the in-class code that instead of a 3 bandit tasks, it is a real N-bandit task, where the choice is simulated among N-bandits, and the reward probabilty is random on the range 0-1 for each bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3txFLwMGh_s_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
