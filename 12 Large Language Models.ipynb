{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1730cff1-ea11-4a3c-b2bc-af24486b9a7f",
   "metadata": {},
   "source": [
    "<a\n",
    "    href=\"https://colab.research.google.com/github/univiemops/tewa1-computational-cognition/blob/main/12%20Large%20Language%20Models.ipynb\"\n",
    "    target=\"_blank\" rel=\"noopener\"> <img\n",
    "      src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "      alt=\"Open In Colab\"> </a>\n",
    "\n",
    "\n",
    "# Tutorial 12 - Large Language Models\n",
    "\n",
    "<img src=\"https://cdn.pixabay.com/photo/2022/12/07/11/48/parrot-7640960_960_720.png\" alt=\"bandit-problem\" style=\"height:300px; background:white\">\n",
    "\n",
    "\n",
    "*Written and revised by Jozsef Arato, Mengfan Zhang, Dominik Pegler, Annika Trapple*  \n",
    "Computational Cognition Course, University of Vienna  \n",
    "https://github.com/univiemops/tewa1-computational-cognition\n",
    "\n",
    "---\n",
    "\n",
    "# This week's lab:\n",
    "\n",
    "This week's assignment will concentrate on the application of large language models (LLMs), specifically focusing on OpenAI's ChatGPT, within psychological research. We will explore two main use-cases: (1) generating synthetic response texts from simulated participants using the ChatGPT API, and (2) analyzing these responses through the same model to derive psychological insights.\n",
    "\n",
    "**Learning Goals:**\n",
    "\n",
    "- How to make API requests\n",
    "- How to create synthetic text data with a LLM\n",
    "- How to analyze a body of text with a LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d2f16-3372-4490-98af-d505210bc3d2",
   "metadata": {},
   "source": [
    "## 0. Imports & Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e11fa883-cc8a-40b6-84ca-726a5d66f5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install openai==1.35.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdbb752a-5c99-4f8c-bad6-0fee2a668198",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from openai import OpenAI # that's default way – if you use a model directly from OpenAI\n",
    "# below is the tewa way, because we use a model that is hosted on microsoft azure\n",
    "from openai import AzureOpenAI as OpenAI\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0a300b-c30f-436f-a7e0-36d4f7b3c868",
   "metadata": {},
   "source": [
    "## 1. Generate Text Data\n",
    "\n",
    "Using a Large Language Model to generate synthetic text data can be beneficial for research, especially in simulation studies. It allows us to test and improve our methods and research tools under controlled conditions, without the ethical and practical constraints of collecting real-world data. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ec52eb-ba8d-47cf-a54b-23274291e542",
   "metadata": {},
   "source": [
    "### 1.1. Set Up the API-Client for the Data Generation Part\n",
    "\n",
    "Request the following credentials from your course instructors and store them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ebbf443-4dd4-4d45-939a-306e1652e9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY = #\n",
    "MODEL = #\n",
    "AZURE_DEPLOYMENT = #\n",
    "AZURE_ENDPOINT = #\n",
    "API_VERSION = #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64859ab0-2273-4724-9906-a74694123f86",
   "metadata": {},
   "source": [
    "### 1.2. Set Up Prompt\n",
    "\n",
    "Your task now is to think of a possible question in a questionnaire, and then, in the next step, to define an instruction for a chatbot to answer that question. Store this instruction as a string in the variable prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c965431-dc1c-4ba0-8dea-f51d66d678cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are TextBot, powered by GPT-4o-Mini, a large language model trained by OpenAI. TextBot focuses its attention on creating text in the same way and style a human would. Since every human is different TextBot is able to create diverse responses. In general TextBot generates rather simple human answers.\"\n",
    "prompt = # YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057feb92-c5bc-4212-b1ee-e3e55e1c7052",
   "metadata": {},
   "source": [
    "### 1.3. Run Text Generation\n",
    "\n",
    "Now create the simulated data for `n_participants` using above prompt. A single request looks like this:\n",
    "\n",
    "```python\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ],\n",
    "            max_tokens=75,\n",
    "            temperature=temperature,\n",
    "        )\n",
    "```\n",
    "\n",
    "You can use a `for` loop to do this. Please use a different/random `temperature` for each participant (see the lecture slides what the temperature parameter is about)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ecf4f2c-f8fb-4b7a-90a0-d1287542f9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_participants = # CHOOSE A NUMBER OF PARTICIPANTS (it is best to start with 1 and once your code works, increase it to something like 50)\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa559e9d-ec8a-47bf-9640-d3baf90ffda6",
   "metadata": {},
   "source": [
    "### 1.4. Inspect Generated Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9dab05-81a6-4ba0-ae45-2e97925d14d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fc64d8-1f7b-47ef-ac0a-bacda39bf7f7",
   "metadata": {},
   "source": [
    "## 2. Topic Modeling\n",
    "\n",
    "Topic modeling is a statistical technique used to identify recurring themes within large bodies of text by detecting common patterns of word co-occurrences. Common methods are Latent Dirichlet Allocation (LDA) and Non-negative Matrix Factorization (NMF). In psychology, we can use topic modeling to analyze free-text responses from surveys to enrich our data analysis. Like this we can automate the extraction of prevalent topics, quantify qualitative data, and gain insights into respondents' mental models. Instead of the common methods mentioned above, we will take a new approach and use a large language model to extract topics from our text dataset.\n",
    "\n",
    "In our example, we want to know what the main topics were in the answers of the simulated participants."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54dec78-2443-400d-9f22-856a8b4d5a44",
   "metadata": {},
   "source": [
    "### 2.1. Set up an API-Client for the analysis part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ec34c-4783-4ce4-bf2e-0bbd137dc1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = # YOUR CODE HERE (see 1.1., it's identical)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a76614e9-7bb8-4676-a03b-eb167e739ca1",
   "metadata": {},
   "source": [
    "### 2.2. Set Up Prompt\n",
    "\n",
    "Your task now is to define a command for your chatbot to return the topics across all text responses as a list of 10 words. It should also assign an importance weight to each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03776cc6-a1c6-4e2e-b440-6fd0db51cdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc5c733-4890-44e0-8efc-32587589eab7",
   "metadata": {},
   "source": [
    "### 2.3. Extract Topics from Text Corpus\n",
    "\n",
    "Send a request to your chatbot similar to before but this time with the new prompt and store the result of the 10 words in the variable `words` and the importance weights in the variable `relevance`. You can use a `temperature` of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4de8b0c-926a-4d71-9a3a-0d5cc9f444a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc047a8d-8593-47a1-bdad-a1e6b710e62e",
   "metadata": {},
   "source": [
    "### 2.4. Create Wordclouds from Topics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504cb8ae-3790-4eb6-8c4d-81894662b767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e990323-4b44-45f1-b9ae-e4d3ecb1ad7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_wordcloud(words, relevance):\n",
    "    word_freq = dict(zip(words, relevance))\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, height=400, background_color=\"white\"\n",
    "    ).generate_from_frequencies(word_freq)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "create_wordcloud(words, relevance)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
