{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5N8uXaLdary"
   },
   "source": [
    "<a\n",
    "    href=\"https://colab.research.google.com/github/univiemops/tewa1-computational-cognition/blob/main/08%20Regression%20with%20Mixed%20Data%20Types.ipynb\"\n",
    "    target=\"_blank\" rel=\"noopener\"> <img\n",
    "      src=\"https://colab.research.google.com/assets/colab-badge.svg\"\n",
    "      alt=\"Open In Colab\"> </a></p>\n",
    "# Tutorial 8 - Regression with mixed data types\n",
    "\n",
    "*Written and revised by Jozsef Arato, Mengfan Zhang, Dominik Pegler*  \n",
    "Computational Cognition Course, University of Vienna  \n",
    "https://github.com/univiemops/tewa1-computational-cognition\n",
    "\n",
    "---\n",
    "## This week's lab:\n",
    "\n",
    "We will introduce you to categorical predictors and logistic regression. A categorical predictor is a variable that represents categories or groups. These groups can be nominal (with no inherent order, such as gender, colour, etc.) or ordinal (with a natural order, such as education level, economic status, etc.). Logistic regression is a statistical technique used to model the probability of a binary outcome based on one or more predictor variables. Categorical predictors and logistic regression can be essential tools when dealing with classification problems in the field of machine learning.\n",
    "\n",
    "**Learning goals:** \\\n",
    "When finishing this tutorial, you should be able to ...\n",
    "* understand categorical Variables and properly encoding them for statistical analysis \n",
    "* explore data with categorical predictors\n",
    "* fit logistic regression and interpret coefficients\n",
    "* understand common performance metrics for evaluating logistic regression models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J-1n3wXvRkNj"
   },
   "source": [
    "## 1. Import libraries and the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xYKPgkYizkzq"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from scipy import io, linalg, stats\n",
    "from sklearn.linear_model import Lasso, LinearRegression, Ridge\n",
    "\n",
    "# import the kid iq dataset\n",
    "response = requests.get(\n",
    "    \"https://ucloud.univie.ac.at/index.php/s/TMnJzRCmKD6ZbYB/download\"\n",
    ")\n",
    "open(\"kid_iq.csv\", \"wb\").write(response.content)\n",
    "\n",
    "df = pd.read_csv(\"kid_iq.csv\")\n",
    "\n",
    "# Gelman, A., Hill, J., & Vehtari, A. (2020).\n",
    "# Regression and Other Stories. Cambridge: Cambridge University Press."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mE1ADoFYSggc"
   },
   "source": [
    "## 2. Inspect the dataset\n",
    "\n",
    "Now, let's first print the data to explore the number of potential predictors, dataset size and the data structure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "egmqytVLSqHc",
    "outputId": "6a144e40-5ac7-4926-c474-a0764b9a3920"
   },
   "outputs": [],
   "source": [
    "print(np.shape(df))\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset contains children's IQ test scores and some information about their mothers. We have three continuous varialbes (*kid_score*, *mom_iq*, *mom_age*) and two categorical variables (*mom_hs* and *mom_work*) in this dataset. \\\n",
    "The *mom_hs* is a binary variable (also known as indicator/dichotomous/dummy variable) that indicates whether the mother graduated from high school (coded as 1) or not (coded as 0). The *mom_work* variable is defined on a four-point ordered scale representing four different work statuses during the first three years of the child's life.\n",
    "\n",
    "Let's have a quick look at how these variables are correlated. Does the correlation match your expectations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "ZFb3azYAjiPt",
    "outputId": "086c0e5a-dd53-4ace-8e6b-2d99c98b2cec"
   },
   "outputs": [],
   "source": [
    "df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nZTrIyQ3Sq6b"
   },
   "source": [
    "We can also visualize some of the data to get a better idea. Instead of using `plt.plot`, we will introduce to another way to create plots using `fig, ax = plt.subplots()` which has several advantages. It returns both the figure (fig) and one or more axes (ax) objects, and you can easily customize both the figure and the individual subplots. We'd like to create a figure with two subplots in the cell below, and we already have the code for the first one, could you complete the code for the second one to plot 'kid_score' against 'mom_hs'? You can customize your plot as you like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "BG5Wg7trSsWz",
    "outputId": "b665c9b9-ea33-46ef-bd70-bd978520adaf"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=2)  # a figure with a 1x2 grid of axes\n",
    "\n",
    "# subplot 1\n",
    "ax[0].scatter(\n",
    "    df[\"mom_iq\"], df[\"kid_score\"], color=\"salmon\", alpha=0.4\n",
    ")  #  plot a scatter on the first axes object\n",
    "ax[0].set_xlabel(\n",
    "    \"mother’s score on an IQ test\"\n",
    ")  # set the x label for the first axes object\n",
    "ax[0].set_ylabel(\"kid's test scores\")\n",
    "\n",
    "# subplot 2\n",
    "# YOUR CORE HERE\n",
    "\n",
    "plt.tight_layout()  # show the figure you created"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "roegl4j1TM1H"
   },
   "source": [
    "## 3. Fit a linear regression using categoriacal predictors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider a linear regression of child test scores on two predictors: the mom's high school\n",
    "indicator and IQ. We will use the `LinearRegression` class from scikit-learn that you learned last week. But feel free to use one of the two other models: `Lasso` and `Ridge`. They are identical to `LinearRegression`, but add penalties to the loss function (sum of squared errors) to reduce overfitting, helping to improve model performance on new data compared to standard `LinearRegression`. The only thing you have to do here is choose the parameter `alpha` for this penalty.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html  \n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[[\"mom_hs\", \"mom_iq\"]]\n",
    "y = df[\"kid_score\"]\n",
    "\n",
    "lr = # your model here\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the coefficients of the predictors and the model fits. How would you interpret the results of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v1f69NrCTrS-",
    "outputId": "7f5ae23b-a8a3-46a9-a7d2-a47e396c2680"
   },
   "outputs": [],
   "source": [
    "print(\"Intercept:\", lr.intercept_)\n",
    "print(\"Slopes:\", lr.coef_)\n",
    "print(\"Score: \", lr.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fitted linear regression above has the form of *kid_score = 25.73 + 5.95 ∗ mom_hs + 0.56 ∗ mom_iq + error*. t. Could you answer the following questions?\n",
    "1. If a child had a mother with an IQ of 0 and who did not finish high school, how would you predict the child's test score?\n",
    "2. If a child had a mother with an IQ of 0 who graduated from high school (just an assumption, I don't mean that anyone without an IQ could graduate from high school), how would you predict this child's test score?\n",
    "3.  Comparing children with the same value of mom_hs but whose mothers differ by 2 points in IQ, what difference would you expect to see in the child's test score?\n",
    "\n",
    "Write down your answers and keep two decimal places in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = # YOUR ANSWER HERE\n",
    "q2 = # YOUR ANSWER HERE\n",
    "q3 = # YOUR AMSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZVCEx_OSTvMj"
   },
   "source": [
    "In the cell below, we visualize the predictions and the original data. We plot two separate lines based on the 'mom_hs' predictor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "id": "CWIi6lriT5st",
    "outputId": "264266d1-b5a4-4da1-9452-c7a692cf8b98"
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"mom_iq\"], df[\"kid_score\"])\n",
    "ax.plot(\n",
    "    df.loc[df[\"mom_hs\"] == 1, \"mom_iq\"],\n",
    "    y_pred[df[\"mom_hs\"] == 1],\n",
    "    color=\"r\",\n",
    "    label=\"no_hs\",\n",
    ")\n",
    "ax.plot(\n",
    "    df.loc[df[\"mom_hs\"] == 0, \"mom_iq\"],\n",
    "    y_pred[df[\"mom_hs\"] == 0],\n",
    "    color=\"g\",\n",
    "    label=\"hs\",\n",
    ")\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"mother’s score on an IQ test\")\n",
    "ax.set_ylabel(\"kid's test scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "APYGuiZPTi4V"
   },
   "source": [
    "### Model with interaction\n",
    "As you can see from the plot above, the slopes of the regression of the child's test score on the mother's IQ were the same for two groups. We can include an interaction between *mom_hs* and *mom_iq* (a third predictor defined as the product of these two variables) to allow the slope to vary between subgroups. Below we create a new design matrix that includes a column for the interaction term and we will fit the linear regression again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pYW2RFEFT_Zm",
    "outputId": "31192d1e-4303-4cf6-e621-a060cc6e442e"
   },
   "outputs": [],
   "source": [
    "df[\"mom_hs_iq_inter\"] = df[\"mom_iq\"] * df[\"mom_hs\"]\n",
    "X_with_inter = df[[\"mom_hs\", \"mom_iq\", \"mom_hs_iq_inter\"]]\n",
    "\n",
    "print(\"Design matrix with interaction:\")\n",
    "print(X_with_inter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_with_inter, df[\"kid_score\"])\n",
    "print(\"Intercept:\", lr.intercept_)\n",
    "print(\"Slopes:\", lr.coef_)\n",
    "print(\"Score:\", lr.score(X_with_inter, df[\"kid_score\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we visulize the plot for predictions again, but this time we have the interaction term included. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "id": "C4tWoZOcyV7V",
    "outputId": "37c1c7df-e6b2-4969-bb43-0b977740b508"
   },
   "outputs": [],
   "source": [
    "education = [\"no_hs\", \"hs\"]\n",
    "colors = [\"r\", \"g\"]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(df[\"mom_iq\"], df[\"kid_score\"])\n",
    "\n",
    "for idx, itm in enumerate(education):\n",
    "    prediciton = lr.predict(X_with_inter[X_with_inter[\"mom_hs\"] == idx])\n",
    "    ax.plot(\n",
    "        X_with_inter.loc[X_with_inter[\"mom_hs\"] == idx, \"mom_iq\"],\n",
    "        prediciton,\n",
    "        label=itm,\n",
    "        color=colors[idx],\n",
    "    )\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"mother’s score on an IQ test\")\n",
    "ax.set_ylabel(\"kid's test scores\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great that we have different slopes for the subgroup regression lines. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we have discussed a bit about categorical predictors, which is a type of input variables that represents categorical data. What if our output variable is categorical? We can perform a logistic regression for this situation. \n",
    "\n",
    "The logistic regression model uses the logistic (sigmoid) function to squeeze the output of a linear equation between 0 and 1. $$ P(Y=1|X) = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n)}} $$\n",
    "\n",
    "Now, let's first define a sigmoid function by ourselves. The function should take as input *x*, where *x = $\\beta_0 + \\beta_1X_1 + \\beta_2X_2 + \\cdots + \\beta_nX_n$* (the linear equation we are familiar with) and return *1/(1+e<sup>-x</sup>)*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can test this function with x = -5 to 5 and take a look at the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-5, 5, 20)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, my_sigmoid(x))\n",
    "ax.set_xlabel(\"x\", fontsize=15)\n",
    "ax.set_ylabel(\"Sigmoid(x)\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! All the function maps all the numbers into the range [0, 1]. \n",
    "\n",
    "We will simulate some data for the next steps. We decide to simulate a scenario where students study between 0 and 50 hours for the exam, and we want to predict whether the student can pass or fail the exam based on the time spent studying. However, we want the simulated data to be centred on zero again, i.e. students study -25 to +25 hours relative to the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100  # number of students\n",
    "x = np.linspace(-25, 25, n)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, my_sigmoid(x))\n",
    "ax.set_xlabel(\"Relative hours studied\", fontsize=15)\n",
    "ax.set_ylabel(\"P(Success)\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Emmm, looks ok-ish, but we haven't implemented the logistic regression model yet. For the next step, we will insert the linear equation into the sigmoid function so that  *p(Y) = sigmoid(b<sub>0</sub>+b<sub>1</sub>*X)*.\n",
    "\n",
    "We define parameters for the intercept and the slope as what we did before, but now we use them with `my_sigmoid` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = -0.1  # intercept\n",
    "b1 = 1.5  # slope\n",
    "lin_pred = b0 + b1 * x\n",
    "p_y = my_sigmoid(lin_pred)\n",
    "\n",
    "# Visulize the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x, p_y)\n",
    "ax.set_xlabel(\"Relative hours studied\", fontsize=15)\n",
    "ax.set_ylabel(\"P(Success)\", fontsize=15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Testing prediciton of  different slopes\n",
    "Below, we'd like to explore how slopes changes can affect the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0 = 0\n",
    "b1_values = np.linspace(0.1, 3, 10)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "for b1 in b1_values:\n",
    "    ax.plot(x, my_sigmoid(b0 + x * b1), label=np.round(b1, 1))\n",
    "ax.legend()\n",
    "ax.set_xlabel(\"hours studied\")\n",
    "ax.set_ylabel(\"p(Success)\")\n",
    "plt.grid(\"ON\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it yourself. We'd like to find good values for b0 and b1 for an exam where almost everyone who studies less than -10 hours fails, but everyone who studies more than +20 hours passes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have set up and explore the logistic model, we are almost ready to simulate some data as our output variable. We need the numpy `random.binomial` generator for this. Test this function in the cells below, and try to understand what it is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(np.random.binomial(n=1, p=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.binomial(n=1, p=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.binomial(n=1, p=0.5, size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, let's start the simulation. The first thing we need to do is simulate 50 students, whose study time is uniformly distributed between -50 and 50 hours compared to the average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_std = 50\n",
    "x = np.random.uniform(-50, 50, n_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict the probability of passing the exam for each student using the linear equation combined with the sigmoid function and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b0_exam = 1.5\n",
    "b1_exam = 0.12\n",
    "y_pred = my_sigmoid(b0_exam + b1_exam * x)\n",
    "\n",
    "# Visulization\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(\n",
    "    np.sort(x), np.sort(y_pred)\n",
    ")  # sort variables to avoid the lines jumping back and forth\n",
    "ax.set_xlabel(\"Relative hours studied\", fontsize=14)\n",
    "ax.set_ylabel(\"P(Success)\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the moment, we are ready to use the predicted probabilities to simulate whether the student passed or failed the exam using the `random.binomial` function. In the cell below, we generate data for *y*, which is either 1 (pass) or 0 (fail), based on the value in *y_pred*, and visualise the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = np.zeros(n_std)\n",
    "\n",
    "for i in range(n_std):\n",
    "    y[i] = np.random.binomial(1, y_pred[i])\n",
    "\n",
    "# Visulization\n",
    "fig, ax = plt.subplots()\n",
    "ax.scatter(x, y)\n",
    "ax.plot(\n",
    "    np.sort(x), np.sort(y_pred)\n",
    ")  # sort variables to avoid the lines jumping back and forth\n",
    "ax.set_xlabel(\"Relative hours studied\", fontsize=14)\n",
    "ax.set_ylabel(\"P(Success)\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting logistic regression\n",
    "\n",
    "Our next step is to actually fit a logistic regression to the simulated data. Similar with fitting a linear regression, we will call the `LogisticRegression` class from scikit-learn to fit the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(x.reshape(-1, 1), y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that we use `reshape(-1, 1)` for *x* to solve the dimensionality issue. When fitting the model, the design matrix should have the shape of n_samples * n_features. `reshape(-1, 1)` results in an array with a single column and multiple rows (a column vector)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original array shape:\", x.shape)\n",
    "print(\"Reshaped array shape:\", x.reshape(-1, 1).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again similar with `LinearRegression`, we can print the fitted parameters for the logistic regression, and compare them with our simulated parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Fitted intercept:\", log_reg.intercept_)\n",
    "print(\"Fitted slope:\", log_reg.coef_)\n",
    "print(\"Score: \", log_reg.score(x.reshape(-1, 1), y))\n",
    "\n",
    "print(\"Simulated intercept:\", b0_exam)\n",
    "print(\"Simulated slope:\", b1_exam)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we can use the `predict_proba()` function to check the probability of the sample for each class in the model (i.e., the likelihood of each data point belongs to fail (0) or pass(1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ll = log_reg.predict_proba(x.reshape(-1, 1))\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we have created a variable *lls* to record the probability that each data point belongs to its currently assigned class and visulize the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lls = np.zeros(n_std)\n",
    "lls[y == 0] = ll[y == 0, 0]  # where true y is 0, use first column of ll\n",
    "lls[y == 1] = ll[y == 1, 1]  # where true y is 1, use second column of ll\n",
    "\n",
    "# Visulization\n",
    "fig, ax = plt.subplots()\n",
    "cd = ax.scatter(x, y, c=lls)  # color data by model likelihood\n",
    "ax.plot(\n",
    "    np.sort(x),\n",
    "    my_sigmoid(np.sort(x) * log_reg.coef_[0] + log_reg.intercept_),\n",
    "    label=\"fitted\",\n",
    ")\n",
    "# plt.plot(np.sort(x),my_sigmoid(np.sort(x)*b1gen+b0gen),label='data gen')\n",
    "# plt.legend()\n",
    "fig.colorbar(cd)\n",
    "ax.set_xlabel(\"Hours studied\", fontsize=14)\n",
    "ax.set_ylabel(\"Exam passed\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "First, try to write two functions: one to generate binary outcome data based on the logistic model, and the other one to fit the logistic regression with scikit-learn. You can use the relevant parts of the code above. \n",
    "\n",
    "**1. Create function `make_log_dat`**\n",
    "\n",
    "This function should take three inputs in order: intercept, slope, an array *X* used to make the predictions. The function should first use these three inputs to calculate *P(Y)* for each value of *X*, where *P(Y)* represents predicted probabilities for outcome=1 (same as above). Then for each *p(Y)*, the function simulates a binary outcome and stores all the outcomes in the array *y*.  Make sure that only the array *y* containing the simulated binary outcomes is returned. \n",
    "\n",
    "**2. Create function `fit_log`**\n",
    "\n",
    "This function should take two inputs in order: an array *X* and an array *y*. The function should fit the scikit-learn logistic regression to *X* and *y* (pay attention to the shape of *X*), and output the intercept and slope of from the fitted logistic regression in order. \n",
    "\n",
    "**3. Compare the difference between the simulated data and the fitted model**\n",
    "\n",
    "Once you have the above functions ready, consider again the scenario of students studying (-25 to 25 hours) to prepare for the exam and perform the following simulation:\n",
    "1. change the number of students from 10 to 1010 in steps of 50\n",
    "2. generate an X and y data set for each value of the number of students\n",
    "3. fit the logistic regression to the generated data and observe how this affects the difference between the generated data and the fitted model.\n",
    "4. visualise your results to see how different numbers of students affect the difference between the data generated and the fitted model.\n",
    "\n",
    "Note: You can choose slope and intercepts arbitrarily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
