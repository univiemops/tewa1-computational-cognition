{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iNFwObzKfjiD"
   },
   "source": [
    "# Tutorial 11 - Introduction to Reinforcement Learning: Bandit Problems\n",
    "\n",
    "*Written and revised by Jozsef Arato, Mengfan Zhang, Dominik Pegler*  \n",
    "Computational Cognition Course, University of Vienna  \n",
    "https://github.com/univiemops/tewa1-computational-cognition\n",
    "\n",
    "---\n",
    "# This week's lab:\n",
    "\n",
    "This week's course content focuses on reinforcement learning, specifically the simpler case with only one state: **bandit problems**. These problems are fundamental to understanding reinforcement learning and have strong ties to psychology. For instance, they mirror the psychological processes behind conditioning and how organisms **learn from rewards**. In multiple one-armed-bandit problems, we're faced with multiple actions, each promising uncertain rewards. The tricky part is the **exploration-exploitation dilemma** – deciding whether to try new options (explore) or stick with rewarding past actions (exploit). In this tutorial we will simulate a human agent who tries to win the most money while playing on three bandits for a number of trials.\n",
    "\n",
    "**Learning Goals:**\n",
    "\n",
    "- Defining reward\n",
    "- Understanding action-values $Q$\n",
    "- Implementing strategies to tackle the explore-exploit-tradeoff\n",
    "- Understanding stationarity vs. non-stationarity\n",
    "\n",
    "**Reference:**\n",
    "- Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An\n",
    "  introduction, 2nd ed. : The MIT Press.\n",
    "\n",
    "\n",
    "---\n",
    "<img src=\"https://people.stfx.ca/jdelamer/courses/csci-531/_images/multi-armed-bandit-slots-est-value.drawio.png\" alt=\"bandit-problem\" style=\"width:800px; background:white\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A3Vwr4KNUyUu"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XVadA6CHgJmo"
   },
   "source": [
    "## 1. The Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fbYzPcG0U0BO"
   },
   "source": [
    "### 1.1. Initial Parameters\n",
    "\n",
    "In reinforcement learning, the environment is one of the two main components, the other being the agent itself. It defines and provides the different states, the possible actions that can be taken, and the rewards associated with these actions. This setup allows the agent to interact with and learn from the environment by making decisions and observing the outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zbrSvRabUy5_"
   },
   "outputs": [],
   "source": [
    "n_bandit = 3  # number of bandits\n",
    "actions = list(range(n_bandit))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xm3QiV9NBR0G"
   },
   "source": [
    "### 1.2. The Reward Function\n",
    "\n",
    "The reward function is part of the environment. Let's define it here as `compute_reward()`. E.g., each bandit should give a reward of 1, but the probabilities that the bandits give the reward are different. Write a function that takes in an action (the chosen bandit, e.g., 0 for the first bandit) and returns the reward based on the probabilities. The reward probabilities are 0.8 for bandit 0, 0.2 for bandit 1 and 0.4 for bandit 2. You can use `np.random.binomial()` to determine whether a reward is returned based on a probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M2suzLODBR0G"
   },
   "outputs": [],
   "source": [
    "reward_probs = [0.8, 0.2, 0.4]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wakhAs4rBR0H",
    "outputId": "8a7b730a-948d-4c2f-a7a0-214a96f3d068"
   },
   "outputs": [],
   "source": [
    "# TEST CELL\n",
    "\n",
    "test_rewards = np.zeros(3)\n",
    "for action in range(n_bandit):\n",
    "    for i in range(100_000):\n",
    "        test_rewards[action] = test_rewards[action] + 1 / (i + 1) * (\n",
    "            compute_reward(action, reward_probs) - test_rewards[action]\n",
    "        )\n",
    "\n",
    "correct = (np.array_equal(np.round(test_rewards, 1), [0.8, 0.2, 0.4])) and (\n",
    "    compute_reward(0, reward_probs) in [0, 1]\n",
    ")\n",
    "\n",
    "if correct:\n",
    "    print(\"Well done! Your reward function works as expected.\")\n",
    "else:\n",
    "    raise Exception(\"Uh-oh! Check your code again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XRxbwVUBc1Dc"
   },
   "source": [
    "Now let's test our reward function with a simple simulation to see what reward each bandit gives us on average. If we did it right, it should converge to our initial reward probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 468
    },
    "id": "F9ljjPV_BR0I",
    "outputId": "640eab74-d646-42b0-8f15-cb81c792aa9b"
   },
   "outputs": [],
   "source": [
    "n_sim = 300\n",
    "\n",
    "test_rewards = np.zeros((3, n_sim))\n",
    "\n",
    "for action in range(n_bandit):\n",
    "    for i in range(0, n_sim):\n",
    "        if i == 0:\n",
    "            test_rewards[action][i] = test_rewards[action][i] + 1 / (i + 1) * (\n",
    "                compute_reward(action, reward_probs) - test_rewards[action][i]\n",
    "            )\n",
    "        else:\n",
    "            test_rewards[action][i] = test_rewards[action][i - 1] + 1 / (i + 1) * (\n",
    "                compute_reward(action, reward_probs) - test_rewards[action][i - 1]\n",
    "            )\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "for i, q in enumerate(test_rewards):\n",
    "    plt.plot(q, label=f\"Bandit {i+1}\")\n",
    "\n",
    "plt.legend()\n",
    "plt.title(f\"Average Reward Simulation ({n_sim} Runs)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZMKjL0pxBR0K"
   },
   "source": [
    "## 2. The Agent\n",
    "\n",
    "### 2.1. Action-Values\n",
    "\n",
    "**Value-based methods** (which we will focus on in this tutorial) are the most widely used in reinforcement learning. This means that the agent associates a certain value $Q$ (derived from *quality*) with each action $a$ (= choosing a bandit) and then chooses the action with the highest value (exploit) or any other action (explore). Initially, the agent has no information about which actions promise the highest reward $R$, i.e. the values are the same for all actions. After multiple interactions with the environment, the agent becomes better in estimating the true value of each action. In general, the value of an action is defined as the average, i.e. expected, reward.\n",
    "\n",
    "$$\\large Q(a) \\doteq \\frac{\\text{sum of rewards when taken }a}{\\text{number of times taken }a}$$\n",
    "\n",
    "For repeated updating this can be written more efficiently as a weighted average of the old value and the new reward. Like this we do not have to store all values in memory, but only the old action value $Q_t(a)$ at timestep $t$ and the number of times $a$ was taken.\n",
    "\n",
    "$$\\large\n",
    "Q_{t+1}(a) =  \\left(1 - \\frac{1}{N_{t+1}(a)}\\right) Q_t(a) + \\frac{1}{N_{t+1}(a)} R_{t+1}\n",
    "$$\n",
    "\n",
    "Which in turn can be transformed into an incremental averaging / updating rule:\n",
    "\n",
    "$$\\large\n",
    "Q_{t+1}(a) = Q_{t}(a) + \\frac{1}{N_{t+1}(a)} (R_{t+1} - Q_{t}(a))\n",
    "$$\n",
    "\n",
    "**This way of updating values is super important in reinforcement learning, so it's worth remembering.**\n",
    "\n",
    "****: We will later replace $\\frac{1}{N_{t+1}(a)}$ with the learning rate parameter $\\alpha$ as this enables us to determine how strongly recent rewards should override previous value estimates. The term $R_{t+1}-Q_{t}(a)$ can also be interpreted in terms of the Rescorla-Wagner model as a prediction error (with larger errors causing larger updates). In simple terms, it is the difference between what you expect to happen (the old value) and what actually happens (the reward). If you think of it as a surprise factor, it's how surprised you are by the outcome. The greater the surprise, the more you will update your beliefs about the world (the values).\n",
    "\n",
    "- Rescorla, R.A. & Wagner, A.R. (1972) [A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement](https://www.ualberta.ca/~egray/teaching/Rescorla%20&%20Wagner%201972.pdf), Classical Conditioning II, A.H. Black & W.F. Prokasy, Eds., pp. 64–99. Appleton-Century-Crofts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f8fJWC6MQ9Sf"
   },
   "source": [
    "### 2.2. The Policy\n",
    "\n",
    "The strategy the agent uses to select actions is called **policy** (often abbreviated as $\\pi$) in general. We just learned that our agent's policy is based on action values. But how exactly does the agent choose actions based on action values? One way would be to always choose the action with the highest value, the so-called **greedy policy**, but in the beginning the agent's value estimates are not very precise and a greedy policy would prevent us from exploring other potentially better actions. How much you should exploit and how much you should explore is known as the **exploration vs. exploitation dilemma**: we want to maximize reward (exploit), but we also want to minimize regret (explore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RVLgOVk3BR0L",
    "outputId": "81bf87b5-60d3-42da-c4f7-f6da5732618c"
   },
   "outputs": [],
   "source": [
    "q = np.array([3, 2, 4])\n",
    "\n",
    "\n",
    "def greedy_policy(q):\n",
    "    max_indices = np.flatnonzero(q == q.max())\n",
    "    return np.random.choice(max_indices)\n",
    "\n",
    "\n",
    "for _ in range(15):\n",
    "    print(\"Action\", greedy_policy(q) + 1, \"chosen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWcWUh4zBR0M"
   },
   "source": [
    "#### 2.2.1. Epsilon-Greedy Policy\n",
    "\n",
    "One strategy to balance exploration and exploitation is the **epsilon-greedy** policy. It works by choosing a random action with probability $\\epsilon$ (exploration) and the action with the highets value with probability 1−$\\epsilon$ (exploitation). This ensures that the agent explores different actions occasionally to discover potentially better options while mostly exploiting the current best-known action to maximize rewards.\n",
    "\n",
    "Please implement the epsilon-greedy policy as a function called `eps_greedy_policy()` which takes the array of values `q`  and `epsilon` as inputs and returns an action. Do not use `np.argmax()` to select the action with the highest value (The reason is that if multiple actions have the same maximum estimated value, `np.argmax()` will consistently pick the first action with the maximum value it encounters. This leads to biased behavior and ignores other equally good options.). You can include the above `greedy_policy()` function in your function as it already takes care of this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQ3jPnTUBR0M",
    "outputId": "3df16344-35a2-4efc-d0d4-ee233ab1439c"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZX_X38nVBR0N",
    "outputId": "5cc461d5-c5f5-4c42-8691-b94c8279bbdc"
   },
   "outputs": [],
   "source": [
    "# TEST CELL\n",
    "\n",
    "test_selections = np.zeros(20_000)\n",
    "for i in range(test_selections.shape[0]):\n",
    "    test_selections[i] = eps_greedy_policy(np.array([3, 3, 1]), 0.1)\n",
    "props = (np.array([np.sum(test_selections == i) for i in range(3)]) / 20_000).round(2)\n",
    "if np.all(np.isclose(props, [0.49, 0.49, 0.03], atol=0.01)):\n",
    "    print(\"Well done! Your e-greedy function works as expected.\")\n",
    "else:\n",
    "    raise Exception(\"Uh-oh! Check your code again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W51K5uGYY3xd"
   },
   "source": [
    "## 3. Learning from Reward\n",
    "\n",
    "So we have initial action values and we have a policy to choose actions based on these values. But our agent still does not learn anything. This means that we keep choosing actions with the same probabilities because our action values stay the same. In the next step, we will implement that the agent receives a reward from the environment after each action and thus can update the action values (and the policy).\n",
    "\n",
    "Here is our update rule ( that this is actually the same as equation 3 we have seen in **2.1.**):\n",
    "\n",
    "$$\\large\n",
    "Q(a) \\leftarrow Q(a) + \\alpha (R - Q(a))\n",
    "$$\n",
    "\n",
    "$R$ is the reward the agent received after taking action $a$, and $\\alpha$ (alpha) is the **learning rate**, set to values between 0 and 1. The learning rate determines the magnitude of the update, with higher values of $\\alpha$ giving more weight to recent rewards."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5Z4d0aEBR0O"
   },
   "source": [
    "### 3.1. Single Learning Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "z47TCGjv8W45",
    "outputId": "b8ac0721-ce81-40a1-ccaf-6330d03cba67"
   },
   "outputs": [],
   "source": [
    "q = np.array([0.0, 0.0, 0.0])  # initial value estimates\n",
    "alpha = 0.5  # learning rate\n",
    "\n",
    "print(\"initial values:\", q)\n",
    "\n",
    "a = eps_greedy_policy(q, 0.1)\n",
    "print(\"action taken:\", a + 1)\n",
    "reward = compute_reward(a, reward_probs)\n",
    "print(\"reward:\", reward)\n",
    "q[a] = q[a] + alpha * (reward - q[a])  # HERE IS THE UPDATE\n",
    "print(\"updated values:\", q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSXEXaLmanGX"
   },
   "source": [
    "### 3.2. Learning Over Many Trials\n",
    "\n",
    "Now let's put this all together for a simulation experiment with multiple trials. You can use a `for`-loop to iterate over `n_trials`. To analyze and visualize the learning behavior of your agent you need to store the action and the updated q array in two arrays called `action_history` and `q_history`. Take care that you initialize these arrays with the correct shape. If your code works, the following visualization should run without any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNU-4ekrBR0Q"
   },
   "outputs": [],
   "source": [
    "n_trials = 1_000  # number of trials\n",
    "q = np.array([0.5, 0.5, 0.5])  # initialize starting values uniformly\n",
    "alpha = 0.01  # learning rate\n",
    "epsilon = 0.2  # exploration parameter\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "k6QzZHaQBR0Q",
    "outputId": "b12fdf4b-8f9b-4551-d4d3-d4b369d0ddea"
   },
   "outputs": [],
   "source": [
    "# Plot setup\n",
    "fig, ax = plt.subplots(2, 1, figsize=(9, 6))\n",
    "colors = [\"steelblue\", \"darksalmon\", \"teal\"]\n",
    "\n",
    "# Plot estimated values as lines\n",
    "for j in range(len(q)):\n",
    "    ax[0].plot(range(n_trials), q_history[:, j], color=colors[j], label=f\"Action {j+1}\")\n",
    "\n",
    "# Plot actions taken as scatter\n",
    "for j in range(len(q)):\n",
    "    ax[1].scatter(\n",
    "        np.where(action_history == j),\n",
    "        action_history[action_history == j] + 1,\n",
    "        color=colors[j],\n",
    "        marker=\"x\",\n",
    "        s=25,\n",
    "        label=f\"Action {j+1}\",\n",
    "    )\n",
    "\n",
    "ax[0].set_ylabel(\"Estimated Value\")\n",
    "ax[0].legend()\n",
    "ax[1].set_ylabel(\"Action Taken\")\n",
    "ax[1].set_yticks(range(1, len(q) + 1))\n",
    "ax[1].set_xlabel(\"Trial\")\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"True values:\", np.array(reward_probs))\n",
    "print(\"Learned values:\", q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lr5yc8XCBR0R"
   },
   "source": [
    "**Q: What do you observe here?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_LYcDIa7BR0R"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MnwV9EUQBR0R"
   },
   "source": [
    "## 4. Simulations\n",
    "\n",
    "### 4.1. Exploring Different Exploration Parameters $\\epsilon$\n",
    "\n",
    "Do a simulation with 5 bandits and try at least three different parameters for $\\epsilon$ (remember: they should be between 0 and 1) and choose one that works best in your simulation. Use a fixed learning rate of `alpha=0.2`, set the number of trials to `n_trials=500`, and use these reward probabilities for your reward function: `[0.1, 0.3, 0.7, 0.2, 0.1]`.\n",
    "\n",
    "Note:\n",
    "\n",
    "- To compare different epsilons you will need to use a performance measure such as **average reward**.   \n",
    "- For a robust result you will need to run **many simulations** (`n_episodes`) each consisting of `n_trials` and average the results.  \n",
    "- A good way to visualize the performance is plotting the trial number on the x-axis and the **average reward per trial** on the y-axis.  \n",
    "\n",
    "Hint: you will need a loop for each epsilon, each episode and each trial. It makes sense to store the rewards and actions taken in arrays of shape `(len(epsilons), n_episodes, n_trials)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "SKc-pKd-BR0S",
    "outputId": "af9e6d8a-3c9e-4c06-8e1f-542b4977d7eb"
   },
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "n_trials = 500  # number of trials\n",
    "alpha = 0.2  # learning Srate\n",
    "epsilons = [0.0, 0.1, 0.3, 0.6, 1.0]\n",
    "reward_probs = [0.1, 0.3, 0.7, 0.2, 0.1]  # [0.5, 0.2, 0.0, 0.3, 0.3]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oknxj-a6BR0T"
   },
   "source": [
    "### 4.2. A New Policy: Softmax\n",
    "\n",
    "Besides $\\epsilon$-greedy, another solution to solve the explore-exploit tradeoff is the **softmax function with temperature parameter**. It converts action values to values between 0 and 1, so we can use them as selection probabilities. This means in practice that the agent selects the actions with the highest values most often, but also explores other actions. The amount of exploration is determined by the temperature parameter (i.e., higher temperature means more exploration).\n",
    "\n",
    "$$\\large p_i = \\frac{\\exp(x_i/\\tau)}{\\sum_{j=1}^{N}\\exp(x_j/\\tau)}$$\n",
    "\n",
    "\n",
    "Let's define the softmax function with temperature parameter for array input ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPfZYUrOQ86u"
   },
   "outputs": [],
   "source": [
    "def softmax(values, temp=1):\n",
    "    \"\"\"\n",
    "    Calculate softmax probabilities.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    values : array-like\n",
    "        Input values.\n",
    "    temp : float, optional\n",
    "        Temperature parameter (default is 1).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    probabilities : ndarray\n",
    "        Softmax probabilities.\n",
    "    \"\"\"\n",
    "    return np.exp(values / temp) / np.sum(np.exp(values / temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4TEdNQpp4Q9d"
   },
   "source": [
    "... and compare different temperature parameters for given action-values $Q(a)$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "id": "rw5ozttuSww3",
    "outputId": "78879771-0649-48bf-94d9-8df5509fbeeb"
   },
   "outputs": [],
   "source": [
    "q = np.array([5, 4, 3])\n",
    "temps = [0.5, 1, 4]\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(12, 4))\n",
    "for i, temp in enumerate(temps):\n",
    "    ps = softmax(q, temp)\n",
    "    for j in range(3):\n",
    "        axs[i].bar(j, ps[j])\n",
    "    axs[i].set_title(f\"Temperature = {temp}\")\n",
    "    axs[i].set_xlabel(\"Action\")\n",
    "    axs[i].set_xticks(range(n_bandit), range(1, n_bandit + 1))\n",
    "    if i == 0:\n",
    "        axs[i].set_ylabel(\"Selection Probability\")\n",
    "    axs[i].set_ylim(0, 1)\n",
    "\n",
    "fig.suptitle(\n",
    "    f\"Action Values $Q$: ({', '.join(q.astype(str))})\",\n",
    ")\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXfYYZQnWttX"
   },
   "source": [
    "Now we are ready to implement our policy, i.e. choosing the action based on the action values. First create an array `q` for our initial estimated action values, make them equal for all three actions, then compute the choice probabilities from these values using the `softmax` function. Once you have the probabilities, you can use `np.random.choice()` to choose an action. To make the action selection procedure reusable, put this all into a function called `softmax_policy()` (which takes `q` as input)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jwJLIUmvWq-2"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NlSriLyIBR0V",
    "outputId": "5eb9ea4e-2ac2-4627-ec34-2933f2dfc7a5"
   },
   "outputs": [],
   "source": [
    "# TEST CELL\n",
    "\n",
    "test_selections = []\n",
    "for _ in range(100):\n",
    "    test_selections.append(softmax_policy(np.array([1, 1, 1]), 1))\n",
    "\n",
    "if set(test_selections) == {0, 1, 2}:\n",
    "    print(\"Well done! Your policy function is ready to be used!\")\n",
    "else:\n",
    "    raise Exception(\"Uh-oh! Please check your code again!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "elm1cdAaBR0V"
   },
   "source": [
    "### 4.3. Exploring Different Temperature Parameters $\\tau$\n",
    "\n",
    "Now do the same simulation as before (you can re-use your code) with the $\\epsilon$-greedy policy, but now try at least three different values for the **temperature parameter** and choose one that works best in your simulation. Use a fixed learning rate of `alpha=0.2`, set the number of trials to `n_trials=500`, and use these reward probabilities for your reward function: `[0.1, 0.3, 0.7, 0.2, 0.1]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 629
    },
    "id": "9qQthZdbBR0V",
    "outputId": "eff32209-4b68-489a-83a3-1cbf791c288e"
   },
   "outputs": [],
   "source": [
    "n_episodes = 100\n",
    "n_trials = 500  # number of trials\n",
    "alpha = 0.2  # learning rate\n",
    "temps = [0.1, 0.3, 1, 3, 8]\n",
    "reward_probs = [0.1, 0.3, 0.7, 0.2, 0.1]  # [0.5, 0.2, 0.0, 0.3, 0.3]\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1BKu5a3CBR0W"
   },
   "source": [
    "## 5. Non-Stationarity\n",
    "\n",
    "So far, we have explored two different algorithms to tackle the explore-exploit tradeoff and found parameters that work well for our given problem. But we only had stationary scenarios, i.e. where the reward probabilities stayed the same. What if the reward probabilities change and a different bandit becomes the best action in the middle of an episode? How would our agents react, would they be able to adapt to this new situation?\n",
    "\n",
    "Run the same simulations as before, but this time we want to\n",
    "\n",
    "- compare our two policies - epsilon-greedy vs. softmax  \n",
    "- compare at least three different values for the learning rate `alpha` (to see which one works best for the two algorithms)  \n",
    "- keep the epsilon and temperature parameters at the values that we found worked best.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZftpiqKgBR0X"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What do you observe?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U-yIAS4wflW7"
   },
   "source": [
    "\n",
    "## 6. Model Experimental Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jeW2xuZf1oC2"
   },
   "source": [
    "In this part, we model humans in a decision task using a softmax policy and want to see if younger participants tend to explore more than older ones. Remember that in the softmax policy this is defined by the temperature parameter $\\tau$.\n",
    "\n",
    "To do this, we first create fake data for you in which human participants had to perform a task with 5 bandits. We do this by using our softmax policy and adding some noise. After you run below cell `data` is created. Inspect it to get a feel for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eC4jwd-OgA77"
   },
   "outputs": [],
   "source": [
    "reward_probs = [0.1, 0.3, 0.7, 0.2, 0.1]\n",
    "\n",
    "\n",
    "def simulate_participant(\n",
    "    n_trials, k_actions=5, tau=1.0, alpha=0.1, reward_probs=reward_probs\n",
    "):\n",
    "    values = np.zeros(k_actions)\n",
    "    action_history = []\n",
    "    reward_history = []\n",
    "    for _ in range(n_trials):\n",
    "        probs = softmax(values, tau)\n",
    "        action = np.random.choice(k_actions, p=probs)\n",
    "        reward = compute_reward(action, reward_probs)\n",
    "        action_history.append(action)\n",
    "        reward_history.append(reward)\n",
    "        values[action] += alpha * (reward - values[action])\n",
    "        # Add some noise to the values\n",
    "        values += np.random.normal(0, 0.1, k_actions)\n",
    "    return action_history, reward_history\n",
    "\n",
    "\n",
    "np.random.seed(0)\n",
    "n_trials = 100\n",
    "participants = 100\n",
    "data = []\n",
    "ages = np.random.randint(12, 50, participants)\n",
    "\n",
    "for age in ages:\n",
    "    tau = 1.5 if age < 18 else 0.5\n",
    "    action_history, reward_history = simulate_participant(\n",
    "        n_trials, k_actions=5, tau=tau, reward_probs=reward_probs\n",
    "    )\n",
    "    data.append(\n",
    "        {\n",
    "            \"age\": age,\n",
    "            \"action_history\": action_history,\n",
    "            \"reward_history\": reward_history,\n",
    "            \"tau\": tau,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fMTQp0e91kFH"
   },
   "source": [
    "For fitting the model, we will use a simple optimization approach to find the best $\\tau$ that matches the observed choices, minimizing the negative log-likelihood of the observed choices given the model. We provide you with the function below and calculate tau for one example participant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1IKGEXOkFd00",
    "outputId": "866c774c-e563-4da3-9d42-74e5bc9f1c9e"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize\n",
    "\n",
    "k_actions = 5\n",
    "\n",
    "participant = 0\n",
    "\n",
    "\n",
    "def rl_fit_log_l(pars, args):\n",
    "    k_actions = 5\n",
    "    alpha = 0.10\n",
    "    tau = pars[0]\n",
    "    actions, rewards = args[0], args[1]\n",
    "    n_trials = len(actions)\n",
    "    likelihood_tr = np.zeros(n_trials)\n",
    "    values = np.zeros(k_actions)\n",
    "    for i in range(n_trials):\n",
    "        action_probs = softmax(values, tau)\n",
    "        likelihood_tr[i] = action_probs[actions[i]]\n",
    "        values[actions[i]] = values[actions[i]] + alpha * (\n",
    "            rewards[i] - values[actions[i]]\n",
    "        )\n",
    "    return -np.sum(np.log(likelihood_tr))\n",
    "\n",
    "\n",
    "action_history = data[participant][\"action_history\"]\n",
    "reward_history = data[participant][\"reward_history\"]\n",
    "real_tau = data[participant][\"tau\"]\n",
    "result = minimize(\n",
    "    rl_fit_log_l,\n",
    "    [1.0],\n",
    "    args=[action_history, reward_history],\n",
    "    bounds=[(0.01, 10)],\n",
    "    method=\"L-BFGS-B\",\n",
    ")\n",
    "estimated_tau = result.x[0]\n",
    "print(f\"Real tau for participant {participant}:      {real_tau:.2f}\")\n",
    "print(f\"Estimated tau for participant {participant}: {estimated_tau:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aU6FHflQ3ljW"
   },
   "source": [
    "Now compute $\\tau$ for all participants. Divide the data into two groups of participants who are 18 or younger and participants who are older than 18. Compute the average $\\tau$ in both groups and run a $t$-test to see if the difference in exploration behavior is significant. You can create a pandas DataFrame from the `data` dictionary and store the `estimated_tau` values in an own column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AfuNSJwt4gn0"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# --------------\n",
    "\n",
    "\n",
    "# --------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q: What is your conclusion?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5j2RWug46u1s"
   },
   "source": [
    "---\n",
    "In this tutorial, we have explored bandit algorithms. They capture the essence of the exploration-exploitation trade-off in reinforcement learning (RL) and serve as a powerful tool for immediate decision-making, especially in single-state environments where the focus is on selecting actions to maximize immediate rewards.\n",
    "\n",
    "Imagine that instead of bandits you now have routes from home to university. One of these routes is the fastest, if everything is running optimally, e.g. no trams or subways are out of service (state 1). If there is only this one state, it is a normal bandit problem, but if we have several states (one could look like this: it is 2 a.m. and only the nightline buses are running), then we are dealing with a complete RL problem with multiple state transitions, where dynamic and sequential decision-making becomes key.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
