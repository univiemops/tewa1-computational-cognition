{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C5N8uXaLdary"
   },
   "source": [
    "# Tutorial 7 - Introduction to scikit-learn\n",
    "\n",
    "*Written and revised by Jozsef Arato, Dominik Pegler*  \n",
    "Computational Cognition Course, University of Vienna  \n",
    "https://github.com/univiemops/tewa1-computational-cognition\n",
    "\n",
    "## This week's lab:\n",
    "\n",
    "In this tutorial, we will introduce you to probably the most important machine learning library in Python: **scikit-learn**. We start by fitting a simple linear model to a well-known dataset (house prices). Then we move on to extensions of the linear model (regularizations) and see if this improves the performance of our model.\n",
    "\n",
    "\n",
    "In this notebook, we have included many explanations as comments in the code cell. Please read them carefully instead of just pressing the run button.  \n",
    "\n",
    "**Learning goals:** \\\n",
    "When you have finished this tutorial, you should be able to ...\n",
    "* fit linear regression models using scikit-learn\n",
    "* check the quality of the model fit\n",
    "* apply feature selection\n",
    "* implement train-test splits to mitigate overfitting and assess the generalization performance of your model on unseen data\n",
    "\n",
    "**Deadline:** Day of next lab, 10:00"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "orxKXnQAKV27"
   },
   "source": [
    "## Import libraries and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7RCNIZnvLStX"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy import linalg, stats\n",
    "\n",
    "# new\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nuOmPYdBYz86"
   },
   "source": [
    "First we download the dataset `real_estate.csv` from the internet, then we read it into a pandas dataframe. As you already know `DataFrame` objects are often abbreviated as `df` (or `df_something`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M6ep27rLk-z"
   },
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    \"https://ucloud.univie.ac.at/index.php/s/crRApfnS4HEm2ar/download\"\n",
    ")\n",
    "open(\"real_estate.csv\", \"wb\").write(response.content)\n",
    "\n",
    "df = pd.read_csv(\"real_estate.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Inspect data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yduEsLFnTcP8"
   },
   "source": [
    "This dataset contains various variables for real estate properties such as transaction date, age, geographic location, etc. The market historical data set of real estate valuation is collected from Sindian Dist., New Taipei City, Taiwan. We want to see how well these variables predict the price of the house. \n",
    "\n",
    "\n",
    "Here is the list of our predictor variables:\n",
    "\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "- X1 ... the transaction date in decimal (e.g., 2013.25=2013 March, 2013.5=2013 June, etc.)\n",
    "- X2 ... the house age (unit: year)\n",
    "- X3 ... the distance to the nearest MRT (metro) station (unit: meter)\n",
    "- X4 ... the number of convenience stores in the living circle on foot (integer)\n",
    "- X5 ... the geographic coordinate, latitude. (unit: degree)\n",
    "- X6 ... the geographic coordinate, longitude. (unit: degree)\n",
    "</div>\n",
    "\n",
    "And this is our outcome variable:\n",
    "<div class='alert alert-info'>\n",
    "\n",
    "- Y ... house price of unit area (10,000 New Taiwan Dollar/Ping, where Ping is a local unit, 1 Ping = 3.3 meter squared)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we want to look at the different variables and calculate the mean, minimum and maximum values for each of them. This will give us a rough idea of our data. Hint: You can use the methods from the pandas tutorial, but there is also a way to `describe()` your data with only one method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 485
    },
    "id": "i1yfiukgKhjY",
    "outputId": "339a7bcd-a84b-4553-94d6-060e8d5a792f"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next step we want to create a list of all variable names with the name `vars` using the DataFrame. We will use this list later. Remember the last tutorial about pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uyZ6Jv09NYo-",
    "outputId": "7808fec1-82dd-4925-e801-b8e5d4d828e4"
   },
   "outputs": [],
   "source": [
    "vars = # YOUR CODE HERE\n",
    "\n",
    "print(vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h1XjInu5KzUh"
   },
   "source": [
    "## 2. Data exploration\n",
    "\n",
    "### 2.1. Explorative data visualization\n",
    "\n",
    "Visualize the data with scatter plots (for the X1-X6 predictors separately). Note that we are adding `tight_layout()` at the end to conventiently make space for all our labels.\n",
    "\n",
    "<div class='alert alert-warning'>This plot can be improved a bit. For example, you can change the size and use the <tt>alpha</tt> argument within <tt>plt.scatter()</tt> to get a more pronounced visualization.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 410
    },
    "id": "NK0BxAJCSWGM",
    "outputId": "d4ed48bf-ee45-4728-f95c-3352352d22e3"
   },
   "outputs": [],
   "source": [
    "plt.figure(\n",
    "    # figsize=\n",
    ")\n",
    "\n",
    "for i, var in enumerate(vars[1:7]):\n",
    "    plt.subplot(\n",
    "        2, 3, i + 1\n",
    "    )  # This means: 2 rows, 3 columns, and current index is i + 1\n",
    "    plt.scatter(df[var], df[vars[7]])\n",
    "    plt.xlabel(var)\n",
    "    plt.ylabel(vars[7])\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0nDPDZDpQeS"
   },
   "source": [
    "### 2.2. Correlation between predictors\n",
    "\n",
    "Two ways of doing this:\n",
    "\n",
    "1. Numpy: `np.correlate()` function\n",
    "2. pandas: `.corr()` method\n",
    "\n",
    "Let's choose pandas. In this case we choose only the relevant part of our DataFrame using the `iloc` attribute and append our correlation method to it. By default this gives us the Pearson correlation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE (df.ilo...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S0nDPDZDpQeS"
   },
   "source": [
    "<div class='alert alert-warning'>You can again improve this plot a bit. Here it makes sense to use the <tt>rotation</tt> argument within <tt>plt.xticks().</tt></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 714
    },
    "id": "uKab3s7ocn5Q",
    "outputId": "d96e8e5a-908f-440b-8a2c-517eac3231d1"
   },
   "outputs": [],
   "source": [
    "plt.figure(\n",
    "    # figsize=\n",
    ")\n",
    "\n",
    "offset = 0  # CHOOSE A REASONABLE VALUE\n",
    "\n",
    "plt.pcolor(df.iloc[:, 1:].corr())\n",
    "plt.xticks(np.arange(len(vars) - 1) + offset, vars[1:])\n",
    "plt.yticks(np.arange(len(vars) - 1) + offset, vars[1:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GxsIZSinqK-j"
   },
   "source": [
    "## 3. Linear regression with a single predictor\n",
    "\n",
    "Scikit-learn (or sklearn for short) uses an object-oriented programming style (https://www.geeksforgeeks.org/introduction-of-object-oriented-programming/), i.e. a slightly different syntax than NumPy and Matplotlib (but somewhat similar to pandas with its central DataFrame object). In our next example, we will use only one predictor. We want to know how well age predicts the price of an apartment.\n",
    "\n",
    "### 3.1. Create model and fit to data\n",
    "\n",
    "The first thing we do is decide which model we want to use. For simplicity, we will use scikit-learn's `LinearRegression` class (actually, we could use any other model that can be used for regression, the basic procedure is mostly the same in scikit-learn), which we imported at the beginning of this tutorial, and create an instance of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tMnfA_5bq4xs"
   },
   "source": [
    "Now we are ready to define our data and fit our newly created model to it. A common convention is to use uppercase `X` (because it's a matrix) for the predictor variables and `y` for the outcome variable. You are free to define this any way you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P51mDA_oq6rF",
    "outputId": "eedec6af-1b7b-4f4d-81c9-e150d498eb9e"
   },
   "outputs": [],
   "source": [
    "X = df[[\"X2 house age\"]]\n",
    "y = df[\"Y house price of unit area\"]\n",
    "\n",
    "lr.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You probably recognized, that we used two pairs of square brackets for `X`, this ensures that it is still a 2D matrix (i.e., a DataFrame). This is necessary for the regression to work, since we could theoretically have more than one predictor. For `y`, we used one pair of square brackets and the result is a pandas Series (vector). This will be clearer if you look at their underlying NumPy structure by using the `values` attribute."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TlMFt_w4q7Dv"
   },
   "source": [
    "### 3.2. Parameters\n",
    "\n",
    "Parameter estimation is a common goal in computational modeling, where the aim is to find the values of model parameters that best fit observed data or match experimental results. Interpretating these parameters, analyzing how changes in each parameter affect the model's predictions or dynamics, can provide insights into the real-world phenomenon the model represents.\n",
    "\n",
    "Let's see the fit parameters intercept and coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9JfYr31Aq9Xv",
    "outputId": "3805ae37-e00d-40ea-cad7-a571a40677de"
   },
   "outputs": [],
   "source": [
    "lr.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6C-m3_3Up01o",
    "outputId": "c4eeeea7-852c-4e5d-db6d-b9499e9e5435"
   },
   "outputs": [],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline(\n",
    "    y=lr.intercept_,\n",
    "    color=\"gray\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Intercept: {lr.intercept_:.2f} price units at age 0\",\n",
    ")\n",
    "\n",
    "y_slope = lr.coef_[0] * np.array([0, 1]) + lr.intercept_\n",
    "plt.plot(\n",
    "    np.array([0, 1]),\n",
    "    y_slope,\n",
    "    color=\"red\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Slope: {lr.coef_[0]:.2f} price units per year\",\n",
    ")\n",
    "\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Price\")\n",
    "plt.title(\"Linear Regression Parameters\")\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HoYJeBe9rKjC"
   },
   "source": [
    "To get a sense of how well our model fits, we can use the coefficient of determination, or R². Here it is simply called `score()`. The R² value measures the proportion of variance in the dependent variable (price) that is explained by the independent variable(s) (age) in the model. It can be interpreted as how well the independent variable(s) (age) can predict changes in the dependent variable (price). It ranges from 0 to 1 (perfect fit). In some cases, however, it can be negative, e.g., when the fit is very poor and your are evaluating on data other than that to which the model was fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RQUnPWyyrOtG",
    "outputId": "acda671a-30a8-448b-beaf-a87bb954eca9"
   },
   "outputs": [],
   "source": [
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class='alert alert-warning'>Only in the case of linear regression with 1 predictor Coefficient of determination R² = (Correlation coefficient r)². Does your score align with your previously computed correlations?</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEoPYCTOsWJN"
   },
   "source": [
    "### 3.4. Prediction of using regression model\n",
    "\n",
    "In addition to parameter estimation, prediction can be a goal of computational modeling (and is the predominant goal in machine learning) by using the model to predict future outcomes based on observed data and estimated parameters.\n",
    "\n",
    "We will make use of the `predict()` method. In the case of a simple model like linear regression, we could calculate the predictions by hand because we only have two parameters (e.g., `lr.intercept_ + lr.coef_ * X_new`). But it's good to get familiar with this method as in more complex models it may become impossible to compute the predictions manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8qWE0Rr_sYXE",
    "outputId": "dcb1e4b3-d30f-4257-c452-74b9a99fca4a"
   },
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X)\n",
    "\n",
    "y_pred[:5]  # A quick sanity check with the first 5 predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xFT9eswOrwLU"
   },
   "source": [
    "Now it's your turn to visualize the prediction line using Matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "id": "1oZPxAGXr1n4",
    "outputId": "f2d240e3-f769-4826-a6e0-1db100f84f69"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes it's good to go slowly and learn more thoroughly, but sometimes it makes sense to go quickly to get results. If you understand Matplotlib well, it is extremely flexible and you can use it to visualize almost anything. But there is another library that helps you get results quickly: **seaborn** ( https://seaborn.pydata.org/). Seaborn creates a bunch of ready-made Matplotlib plots with fewer lines of code. Here is an example for our regression problem:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.regplot(x=X, y=y, line_kws=dict(color=\"firebrick\"), scatter_kws=dict(alpha=0.3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With only one line of code you plot your data, labels, the regression line, and even a 95% confidence interval around it. While it is good to learn Matplotlib from the ground up, to get things done it is often helpful to use a higher level interface like seaborn that saves us from writing a lot of code. If you want to have more control over your plots we encourage you to stick with Matplotlib."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V6SXo619Nn-N"
   },
   "source": [
    "## 4. Linear regression with more predictors\n",
    "\n",
    "Now it's time to move on and use the four variables X1 through X4 in a combined model. To do this, we create a combined predictor matrix from our original data frame, containing only the predictors we want to use. We can use our `vars` variable that we created earlier ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jQ_av7DzT_xp",
    "outputId": "5d3eaef8-cd4d-48c0-da9f-0ff68eb6725b"
   },
   "outputs": [],
   "source": [
    "X = df[vars[1:5]]\n",
    "y = df[vars[7]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S5eVbOpl86KP"
   },
   "source": [
    "... create a model and fit it to the new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7hNoiy2u9oV",
    "outputId": "53aca782-36ad-488f-ebcd-b48ee1ee9226"
   },
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "lr.fit(X, y)\n",
    "lr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute the R² and see how including more predictors changed the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QBU6wPID9CPs",
    "outputId": "55a0323c-36de-4e04-f147-d72de9f8415a"
   },
   "outputs": [],
   "source": [
    "lr.score(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<div class='alert alert-info'>You can submit this assignment as a group.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tAiw-W0i-SQj"
   },
   "source": [
    "##  Exercise 1: Feature selection\n",
    "\n",
    "Let's now focus on selecting the most important features or variables from our dataset to create our model. Our goal is to improve model performance by minimizing dimensionality and complexity. To achieve this, we will\n",
    "\n",
    " 1. Add the predictors **sequentially**, i.e., first fit a model to only X1, then another model to X1 and X2, until you have included all the features up to X6, store the resulting `score()` (R²) for each of the models.\n",
    " 2. Do the same, but add predictors in a **random** order one-by-one.\n",
    " 3. Do the same, but add predictors in the order of the **pearson correlation** with the outcome variable Y (starting with the largest).\n",
    " 4. Plot the obtained scores from 1-3 as three lines.\n",
    " \n",
    "You can use for-loops to iterate over the different models.\n",
    "\n",
    "Please add a sentence about what you are observing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 688
    },
    "id": "OVSGSP51-fuV",
    "outputId": "d8c7836d-1989-450b-be96-3fa66494d5a3"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rFTFbyrPYrD5"
   },
   "source": [
    "## Exercise 2: Train-test split\n",
    "\n",
    "**Overfitting** is a common problem in machine learning, where the model learns too well from the training data, even picking up noise and irrelevant details, which makes it difficult to generalize to new data. The train-test split helps by splitting the data into two parts: one for training the model and another for testing how well it generalizes to new data. There is a built-in train-test-split function in scikit-learn, but we want you to build your own. To achieve this, we will:\n",
    "\n",
    "1. Split the `X` and `y` data into a 80% training (`X_train`, `y_train`) and a 20% test set (`X_test`, `y_test`).\n",
    "\n",
    "- Option 1: Simply use the first 80% of rows for the training set and the rest for the test set). You can use indexing for this: e.g., `df[0:int(len(Data)*.8)]` selects the first 80% percent of a numpy array.\n",
    "\n",
    "- Option 2: Randomly select 80% of the data as training, and the rest for test. This is the better approach, but make sure that both X and y are created from the same random selection (e.g., the rows in `X_test` correspond to the rows in `y_test`, ...).\n",
    "\n",
    "\n",
    "2. Fit the model to the training set, and compute the `score()` both for the training and the test set\n",
    "\n",
    "3. Similar to the previous exercise, try to find the best combination of predictors that best explain the test set and compute the `score()` for both training set and test set.\n",
    "\n",
    "4. Try to visualize the last results in a similar way like in Exercise 1 (one plot with a line for training set and a line for the test set).\n",
    "\n",
    "Please add a sentence about what you are observing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jgq8yoCbY7h2"
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
